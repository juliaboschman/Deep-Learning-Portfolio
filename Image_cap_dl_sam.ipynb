{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZSQ8-H9q19l",
    "outputId": "01ada965-527e-4c62-82a3-3f42936405c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "  !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUdCJPbNrBfq",
    "outputId": "7ed0add8-7d59-471a-f8a6-aad2f4648684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.30.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.40)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cN-AB3nNkPWV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from transformers import ViTFeatureExtractor, VisionEncoderDecoderModel, AutoProcessor, ViTImageProcessor\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v16Cc4mMst3u",
    "outputId": "1cc63dc1-1cd4-4c48-a585-c66c53c24091"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"tomytjandra/h-and-m-fashion-caption-12k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzS9443c_6-v"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        text = sample[\"text\"]\n",
    "        image = sample[\"image\"]\n",
    "\n",
    "        # Check if image is a PIL Image object\n",
    "        if isinstance(image, Image.Image):\n",
    "            # Apply transformations if provided\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        else:\n",
    "            print(\"Error: Unexpected image format.\")\n",
    "            return None\n",
    "\n",
    "        return image, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31jYA6w4mp2g",
    "outputId": "e5c15b21-7a50-47f0-f770-66c565a39c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 9949\n",
      "Validation data size: 2488\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Assuming `dataset` is the original dataset loaded as a DatasetDict\n",
    "dataset_dict = dataset['train']\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Perform the split\n",
    "dataset_split = dataset_dict.train_test_split(test_size=0.2, seed=seed)\n",
    "\n",
    "# Separate the split datasets\n",
    "train_data = dataset_split['train']\n",
    "val_data = dataset_split['test']\n",
    "\n",
    "# Check the number of samples\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5tcUCZZAEOI"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the dataset with transformations\n",
    "transform = transforms.Compose([\n",
    "    # Add your desired transformations here\n",
    "    transforms.Resize((224, 224)),  # Example transformation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "train_dataset = ImageCaptioningDataset(train_data, transform=transform)\n",
    "val_dataset = ImageCaptioningDataset(val_data, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=30, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=30, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_Mt8eJ6doVX",
    "outputId": "9fdf13d4-3ee5-4b66-ac25-4cc72b755960"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [0/332], Loss: 5.6277\n",
      "Epoch [1/5], Batch [10/332], Loss: 2.7620\n",
      "Epoch [1/5], Batch [20/332], Loss: 2.1261\n",
      "Epoch [1/5], Batch [30/332], Loss: 1.9173\n",
      "Epoch [1/5], Batch [40/332], Loss: 1.7266\n",
      "Epoch [1/5], Batch [50/332], Loss: 1.6254\n",
      "Epoch [1/5], Batch [60/332], Loss: 1.3636\n",
      "Epoch [1/5], Batch [70/332], Loss: 1.5087\n",
      "Epoch [1/5], Batch [80/332], Loss: 1.3350\n",
      "Epoch [1/5], Batch [90/332], Loss: 1.4003\n",
      "Epoch [1/5], Batch [100/332], Loss: 1.2501\n",
      "Epoch [1/5], Batch [110/332], Loss: 1.3001\n",
      "Epoch [1/5], Batch [120/332], Loss: 1.2976\n",
      "Epoch [1/5], Batch [130/332], Loss: 1.3262\n",
      "Epoch [1/5], Batch [140/332], Loss: 1.1560\n",
      "Epoch [1/5], Batch [150/332], Loss: 1.2324\n",
      "Epoch [1/5], Batch [160/332], Loss: 1.1827\n",
      "Epoch [1/5], Batch [170/332], Loss: 1.2151\n",
      "Epoch [1/5], Batch [180/332], Loss: 1.1972\n",
      "Epoch [1/5], Batch [190/332], Loss: 1.2164\n",
      "Epoch [1/5], Batch [200/332], Loss: 1.0743\n",
      "Epoch [1/5], Batch [210/332], Loss: 1.1509\n",
      "Epoch [1/5], Batch [220/332], Loss: 1.0999\n",
      "Epoch [1/5], Batch [230/332], Loss: 1.0992\n",
      "Epoch [1/5], Batch [240/332], Loss: 1.0484\n",
      "Epoch [1/5], Batch [250/332], Loss: 1.1074\n",
      "Epoch [1/5], Batch [260/332], Loss: 1.0176\n",
      "Epoch [1/5], Batch [270/332], Loss: 1.0458\n",
      "Epoch [1/5], Batch [280/332], Loss: 1.0280\n",
      "Epoch [1/5], Batch [290/332], Loss: 1.1118\n",
      "Epoch [1/5], Batch [300/332], Loss: 0.9883\n",
      "Epoch [1/5], Batch [310/332], Loss: 1.1427\n",
      "Epoch [1/5], Batch [320/332], Loss: 1.0487\n",
      "Epoch [1/5], Batch [330/332], Loss: 1.0395\n",
      "Epoch 1/5, Average Training Loss: 1.3748\n",
      "Epoch 1/5, Average Validation Loss: 1.0843\n",
      "Epoch [2/5], Batch [0/332], Loss: 1.0095\n",
      "Epoch [2/5], Batch [10/332], Loss: 0.9137\n",
      "Epoch [2/5], Batch [20/332], Loss: 0.9263\n",
      "Epoch [2/5], Batch [30/332], Loss: 0.9598\n",
      "Epoch [2/5], Batch [40/332], Loss: 1.0966\n",
      "Epoch [2/5], Batch [50/332], Loss: 1.0613\n",
      "Epoch [2/5], Batch [60/332], Loss: 0.9861\n",
      "Epoch [2/5], Batch [70/332], Loss: 0.9590\n",
      "Epoch [2/5], Batch [80/332], Loss: 1.0122\n",
      "Epoch [2/5], Batch [90/332], Loss: 1.1054\n",
      "Epoch [2/5], Batch [100/332], Loss: 1.0704\n",
      "Epoch [2/5], Batch [110/332], Loss: 0.8750\n",
      "Epoch [2/5], Batch [120/332], Loss: 0.9656\n",
      "Epoch [2/5], Batch [130/332], Loss: 1.0529\n",
      "Epoch [2/5], Batch [140/332], Loss: 1.0182\n",
      "Epoch [2/5], Batch [150/332], Loss: 0.8994\n",
      "Epoch [2/5], Batch [160/332], Loss: 0.9932\n",
      "Epoch [2/5], Batch [170/332], Loss: 0.9611\n",
      "Epoch [2/5], Batch [180/332], Loss: 0.9604\n",
      "Epoch [2/5], Batch [190/332], Loss: 1.0689\n",
      "Epoch [2/5], Batch [200/332], Loss: 0.9525\n",
      "Epoch [2/5], Batch [210/332], Loss: 0.8898\n",
      "Epoch [2/5], Batch [220/332], Loss: 0.9249\n",
      "Epoch [2/5], Batch [230/332], Loss: 1.0490\n",
      "Epoch [2/5], Batch [240/332], Loss: 0.9194\n",
      "Epoch [2/5], Batch [250/332], Loss: 0.9524\n",
      "Epoch [2/5], Batch [260/332], Loss: 1.0290\n",
      "Epoch [2/5], Batch [270/332], Loss: 0.9052\n",
      "Epoch [2/5], Batch [280/332], Loss: 1.0406\n",
      "Epoch [2/5], Batch [290/332], Loss: 0.9556\n",
      "Epoch [2/5], Batch [300/332], Loss: 0.9032\n",
      "Epoch [2/5], Batch [310/332], Loss: 0.8984\n",
      "Epoch [2/5], Batch [320/332], Loss: 0.8653\n",
      "Epoch [2/5], Batch [330/332], Loss: 1.0458\n",
      "Epoch 2/5, Average Training Loss: 0.9525\n",
      "Epoch 2/5, Average Validation Loss: 0.9884\n",
      "Epoch [3/5], Batch [0/332], Loss: 0.8240\n",
      "Epoch [3/5], Batch [10/332], Loss: 0.8548\n",
      "Epoch [3/5], Batch [20/332], Loss: 0.8146\n",
      "Epoch [3/5], Batch [30/332], Loss: 0.7555\n",
      "Epoch [3/5], Batch [40/332], Loss: 0.7844\n",
      "Epoch [3/5], Batch [50/332], Loss: 0.6573\n",
      "Epoch [3/5], Batch [60/332], Loss: 0.6953\n",
      "Epoch [3/5], Batch [70/332], Loss: 0.7584\n",
      "Epoch [3/5], Batch [80/332], Loss: 0.7282\n",
      "Epoch [3/5], Batch [90/332], Loss: 0.7150\n",
      "Epoch [3/5], Batch [100/332], Loss: 0.7408\n",
      "Epoch [3/5], Batch [110/332], Loss: 0.7902\n",
      "Epoch [3/5], Batch [120/332], Loss: 0.6209\n",
      "Epoch [3/5], Batch [130/332], Loss: 0.7620\n",
      "Epoch [3/5], Batch [140/332], Loss: 0.7264\n",
      "Epoch [3/5], Batch [150/332], Loss: 0.6909\n",
      "Epoch [3/5], Batch [160/332], Loss: 0.7634\n",
      "Epoch [3/5], Batch [170/332], Loss: 0.6509\n",
      "Epoch [3/5], Batch [180/332], Loss: 0.8008\n",
      "Epoch [3/5], Batch [190/332], Loss: 0.7090\n",
      "Epoch [3/5], Batch [200/332], Loss: 0.7464\n",
      "Epoch [3/5], Batch [210/332], Loss: 0.6323\n",
      "Epoch [3/5], Batch [220/332], Loss: 0.6793\n",
      "Epoch [3/5], Batch [230/332], Loss: 0.6624\n",
      "Epoch [3/5], Batch [240/332], Loss: 0.7314\n",
      "Epoch [3/5], Batch [250/332], Loss: 0.7376\n",
      "Epoch [3/5], Batch [260/332], Loss: 0.8072\n",
      "Epoch [3/5], Batch [270/332], Loss: 0.6718\n",
      "Epoch [3/5], Batch [280/332], Loss: 0.7661\n",
      "Epoch [3/5], Batch [290/332], Loss: 0.7236\n",
      "Epoch [3/5], Batch [300/332], Loss: 0.7519\n",
      "Epoch [3/5], Batch [310/332], Loss: 0.7447\n",
      "Epoch [3/5], Batch [320/332], Loss: 0.6424\n",
      "Epoch [3/5], Batch [330/332], Loss: 0.7073\n",
      "Epoch 3/5, Average Training Loss: 0.7377\n",
      "Epoch 3/5, Average Validation Loss: 0.9096\n",
      "Epoch [4/5], Batch [0/332], Loss: 0.7600\n",
      "Epoch [4/5], Batch [10/332], Loss: 0.7075\n",
      "Epoch [4/5], Batch [20/332], Loss: 0.6851\n",
      "Epoch [4/5], Batch [30/332], Loss: 0.5826\n",
      "Epoch [4/5], Batch [40/332], Loss: 0.6801\n",
      "Epoch [4/5], Batch [50/332], Loss: 0.6407\n",
      "Epoch [4/5], Batch [60/332], Loss: 0.7682\n",
      "Epoch [4/5], Batch [70/332], Loss: 0.6488\n",
      "Epoch [4/5], Batch [80/332], Loss: 0.6878\n",
      "Epoch [4/5], Batch [90/332], Loss: 0.6643\n",
      "Epoch [4/5], Batch [100/332], Loss: 0.6435\n",
      "Epoch [4/5], Batch [110/332], Loss: 0.7199\n",
      "Epoch [4/5], Batch [120/332], Loss: 0.6984\n",
      "Epoch [4/5], Batch [130/332], Loss: 0.6669\n",
      "Epoch [4/5], Batch [140/332], Loss: 0.7172\n",
      "Epoch [4/5], Batch [150/332], Loss: 0.6616\n",
      "Epoch [4/5], Batch [160/332], Loss: 0.7518\n",
      "Epoch [4/5], Batch [170/332], Loss: 0.6509\n",
      "Epoch [4/5], Batch [180/332], Loss: 0.6542\n",
      "Epoch [4/5], Batch [190/332], Loss: 0.6865\n",
      "Epoch [4/5], Batch [200/332], Loss: 0.7504\n",
      "Epoch [4/5], Batch [210/332], Loss: 0.6937\n",
      "Epoch [4/5], Batch [220/332], Loss: 0.6611\n",
      "Epoch [4/5], Batch [230/332], Loss: 0.6624\n",
      "Epoch [4/5], Batch [240/332], Loss: 0.6874\n",
      "Epoch [4/5], Batch [250/332], Loss: 0.7202\n",
      "Epoch [4/5], Batch [260/332], Loss: 0.7003\n",
      "Epoch [4/5], Batch [270/332], Loss: 0.6836\n",
      "Epoch [4/5], Batch [280/332], Loss: 0.6868\n",
      "Epoch [4/5], Batch [290/332], Loss: 0.7218\n",
      "Epoch [4/5], Batch [300/332], Loss: 0.6858\n",
      "Epoch [4/5], Batch [310/332], Loss: 0.7020\n",
      "Epoch [4/5], Batch [320/332], Loss: 0.6386\n",
      "Epoch [4/5], Batch [330/332], Loss: 0.6787\n",
      "Epoch 4/5, Average Training Loss: 0.6892\n",
      "Epoch 4/5, Average Validation Loss: 0.9055\n",
      "Epoch [5/5], Batch [0/332], Loss: 0.6602\n",
      "Epoch [5/5], Batch [10/332], Loss: 0.6751\n",
      "Epoch [5/5], Batch [20/332], Loss: 0.6579\n",
      "Epoch [5/5], Batch [30/332], Loss: 0.6022\n",
      "Epoch [5/5], Batch [40/332], Loss: 0.6365\n",
      "Epoch [5/5], Batch [50/332], Loss: 0.6362\n",
      "Epoch [5/5], Batch [60/332], Loss: 0.6372\n",
      "Epoch [5/5], Batch [70/332], Loss: 0.6602\n",
      "Epoch [5/5], Batch [80/332], Loss: 0.7091\n",
      "Epoch [5/5], Batch [90/332], Loss: 0.5823\n",
      "Epoch [5/5], Batch [100/332], Loss: 0.6855\n",
      "Epoch [5/5], Batch [110/332], Loss: 0.6577\n",
      "Epoch [5/5], Batch [120/332], Loss: 0.6402\n",
      "Epoch [5/5], Batch [130/332], Loss: 0.7295\n",
      "Epoch [5/5], Batch [140/332], Loss: 0.7049\n",
      "Epoch [5/5], Batch [150/332], Loss: 0.6630\n",
      "Epoch [5/5], Batch [160/332], Loss: 0.6996\n",
      "Epoch [5/5], Batch [170/332], Loss: 0.6669\n",
      "Epoch [5/5], Batch [180/332], Loss: 0.6132\n",
      "Epoch [5/5], Batch [190/332], Loss: 0.5771\n",
      "Epoch [5/5], Batch [200/332], Loss: 0.5961\n",
      "Epoch [5/5], Batch [210/332], Loss: 0.6333\n",
      "Epoch [5/5], Batch [220/332], Loss: 0.6010\n",
      "Epoch [5/5], Batch [230/332], Loss: 0.6864\n",
      "Epoch [5/5], Batch [240/332], Loss: 0.6338\n",
      "Epoch [5/5], Batch [250/332], Loss: 0.6983\n",
      "Epoch [5/5], Batch [260/332], Loss: 0.5988\n",
      "Epoch [5/5], Batch [270/332], Loss: 0.6718\n",
      "Epoch [5/5], Batch [280/332], Loss: 0.6275\n",
      "Epoch [5/5], Batch [290/332], Loss: 0.6290\n",
      "Epoch [5/5], Batch [300/332], Loss: 0.6355\n",
      "Epoch [5/5], Batch [310/332], Loss: 0.5620\n",
      "Epoch [5/5], Batch [320/332], Loss: 0.6631\n",
      "Epoch [5/5], Batch [330/332], Loss: 0.6223\n",
      "Epoch 5/5, Average Training Loss: 0.6454\n",
      "Epoch 5/5, Average Validation Loss: 0.9040\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming device is defined somewhere above\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained pipeline model\n",
    "pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n",
    "pipe.model.to(device)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Define your loss function (e.g., cross-entropy loss)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = torch.optim.Adam(pipe.model.parameters(), lr=5e-5)\n",
    "\n",
    "# Calculate total training steps\n",
    "epochs = 5\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "# Early stopping criteria\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    pipe.model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        images, texts = batch\n",
    "        images = images.to(device)\n",
    "\n",
    "        tokenized_texts = tokenizer(list(texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = tokenized_texts.input_ids.to(device)\n",
    "        labels = tokenized_texts.input_ids.to(device).clone()\n",
    "\n",
    "        labels[:, :-1] = labels[:, 1:].clone()\n",
    "        labels[:, -1] = tokenizer.pad_token_id\n",
    "        attention_mask = tokenized_texts.attention_mask.to(device)\n",
    "\n",
    "        outputs = pipe.model(pixel_values=images, input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    pipe.model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            images, texts = batch\n",
    "            images = images.to(device)\n",
    "\n",
    "            tokenized_texts = tokenizer(list(texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            input_ids = tokenized_texts.input_ids.to(device)\n",
    "            labels = tokenized_texts.input_ids.to(device).clone()\n",
    "\n",
    "            labels[:, :-1] = labels[:, 1:].clone()\n",
    "            labels[:, -1] = tokenizer.pad_token_id\n",
    "            attention_mask = tokenized_texts.attention_mask.to(device)\n",
    "\n",
    "            outputs = pipe.model(pixel_values=images, input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            logits = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "            labels = labels.view(-1)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chc4bIVnaYpH",
    "outputId": "8acc426e-8143-453a-b594-8c9df6abdfc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/Project_4/token/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/Project_4/token/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/Project_4/token/vocab.txt',\n",
       " '/content/drive/MyDrive/Project_4/token/added_tokens.json',\n",
       " '/content/drive/MyDrive/Project_4/token/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model.save_pretrained(\"/content/drive/MyDrive/Project_4/pipe\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/Project_4/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TG8eNZKnTNDh",
    "outputId": "ca7fa798-ddd8-4643-8567-355a0e82af9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvr-hUFtXOhg",
    "outputId": "2341fd2b-b0cb-4943-a4d5-3a8e3c26a164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.4)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=0759b4d1bffb3c3c9ade42745d9e579a4cda16c390392ed7ec0bc0aec34cca2a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 966
    },
    "id": "Dfc9XdacW-Dl",
    "outputId": "0e366043-8e6d-4fd1-df58-b0fcb136d745"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for bleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/bleu/bleu.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for meteor contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/meteor/meteor.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9034\n",
      "Example 1\n",
      "True: solid black trousers in a crepe weave with a low ribbed waistband and straight wide legs\n",
      "Pred: solid black trousers in a softepe weave with a high waistbed waistband and wide wide legs with wide jerseys jersey - - - - jerseysrrsesrersseserssrerserssrsrererersseer\n",
      "Example 2\n",
      "True: solid black jumper in a soft fine-knit viscose blend with a deep v-neck long sleeves and ribbing around the neckline cuffs and hem\n",
      "Pred: solid black jumper in a soft fine knit knit viscose blend with a v v - neck long sleeves and ribbing around the neckline cuffs and hem rerer r knit r fibre r lining fibre rer rer longer lining r longer r longerie r r rer r\n",
      "Example 3\n",
      "True: solid green ankle-length kaftan in a crepe weave with flounces down the sides and a v-neck with a wide flounced trim rounded hem with a flounced trim and short slits in the sides slightly shorter at the front\n",
      "Pred: solid green short - length kaftan in a crepe weave with aounces down the front and a v - neck with a concealed flounce trim short hem slightly short longerounce trim short sleeves in the sides longer at the back - -ss -\n",
      "Example 4\n",
      "True: solid beige short fitted blouse in a cotton weave with shaping seams deep v-neck hook-and-eye fasteners at the front shoulder pads and short sleeves with a press-stud fastening\n",
      "Pred: solid beige short - jacket in a cotton weave with a seams at v - neck at - and - eye fasteners at the front and pads and short wide with se slit - stud atening at lining lininger lining lining lining lining lining lining lining liningieer linings lining lining\n",
      "Example 5\n",
      "True: solid black jumper in fine-knit cotton with a round neck long sleeves chest pocket and ribbing around the neckline cuffs and hem\n",
      "Pred: solid black jumper in a - knit cotton with a round neckline sleeves and pocket and rollbing around the neckline cuffs and hem long r r r r r r long r long r r r r r r r long long r long r long r r r r r r\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got a string but expected a list instead: 'solid black trousers in a softepe weave with a high waistbed waistband and wide wide legs with wide jerseys jersey - - - - jerseysrrsesrersseserssrerserssrsrererersseer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f4ad93461dd8>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Compute and print the evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f4ad93461dd8>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(predictions, references)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     results = {\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;34m\"bleu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"meteor\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmeteor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m\"rouge\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrouge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/metric.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/metric.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"predictions\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"references\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mintput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mintput_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mintput_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m             \u001b[0mencoded_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m             \u001b[0mencoded_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;31m# schema.feature is not a dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# don't interpret a string as a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got a string but expected a list instead: '{obj}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Got a string but expected a list instead: 'solid black trousers in a softepe weave with a high waistbed waistband and wide wide legs with wide jerseys jersey - - - - jerseysrrsesrersseserssrerserssrsrererersseer'"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "# Define the evaluation metrics\n",
    "bleu = load_metric(\"bleu\")\n",
    "meteor = load_metric(\"meteor\")\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    results = {\n",
    "        \"bleu\": bleu.compute(predictions=predictions, references=references),\n",
    "        \"meteor\": meteor.compute(predictions=predictions, references=references),\n",
    "        \"rouge\": rouge.compute(predictions=predictions, references=references),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "# Generate predictions and references and compute validation loss\n",
    "pipe.model.eval()\n",
    "total_val_loss = 0.0\n",
    "predictions = []\n",
    "references = []\n",
    "val_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        images, texts = batch\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Tokenize texts\n",
    "        tokenized_texts = tokenizer(list(texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Shift input_ids and labels for language modeling\n",
    "        input_ids = tokenized_texts.input_ids.to(device)\n",
    "        labels = tokenized_texts.input_ids.to(device).clone()\n",
    "        labels[:, :-1] = labels[:, 1:].clone()\n",
    "        labels[:, -1] = tokenizer.pad_token_id\n",
    "        attention_mask = tokenized_texts.attention_mask.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = pipe.model(pixel_values=images, input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Compute the loss\n",
    "        logits = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "        # Collect predictions\n",
    "        pred_texts = tokenizer.batch_decode(torch.argmax(outputs.logits, dim=-1), skip_special_tokens=True)\n",
    "        val_predictions.extend(zip(texts, pred_texts))\n",
    "\n",
    "        # For metric calculation\n",
    "        predictions.extend(pred_texts)\n",
    "        references.extend([[text] for text in texts])  # BLEU expects a list of references for each prediction\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Print a few validation examples\n",
    "    for i, (true_text, pred_text) in enumerate(val_predictions[:5]):  # Display first 5 examples\n",
    "        print(f\"Example {i+1}\")\n",
    "        print(f\"True: {true_text}\")\n",
    "        print(f\"Pred: {pred_text}\")\n",
    "\n",
    "# Compute and print the evaluation metrics\n",
    "metrics = compute_metrics(predictions, references)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
